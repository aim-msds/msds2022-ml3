{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "092f6dd3-620d-477e-a539-c904fff0eeaa",
   "metadata": {},
   "source": [
    "# Word-level RNN as a Language Model\n",
    "\n",
    "*Prepared by Sebastian C. Iba√±ez*\n",
    "\n",
    "--- \n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/aim-msds/msds2022-ml3/blob/main/notebooks/05_RNN/02_word-rnn.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"float: left;\"></a><br>\n",
    "\n",
    "In this notebook, our goal is to create a language model by implementing a word-level RNN.\n",
    "\n",
    "For reference, here's the architecture we want to build:\n",
    "\n",
    "<img src=\"images/wordlevel-rnn.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c4b8c9c-e333-49c8-bcc2-a61a94ed39e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:32:23.959408Z",
     "iopub.status.busy": "2022-01-26T09:32:23.959408Z",
     "iopub.status.idle": "2022-01-26T09:32:25.439820Z",
     "shell.execute_reply": "2022-01-26T09:32:25.439820Z",
     "shell.execute_reply.started": "2022-01-26T09:32:23.959408Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edc9e67-665f-4e3b-9ffe-8092bd87356c",
   "metadata": {},
   "source": [
    "Let's look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a91925e-5f49-4c7a-9322-89d3fa45ad6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:32:25.440822Z",
     "iopub.status.busy": "2022-01-26T09:32:25.440822Z",
     "iopub.status.idle": "2022-01-26T09:32:25.455835Z",
     "shell.execute_reply": "2022-01-26T09:32:25.455835Z",
     "shell.execute_reply.started": "2022-01-26T09:32:25.440822Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "path_to_file = 'data/shakespeare.txt'\n",
    "\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8', errors='ignore')\n",
    "\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f70113-7c3e-4375-86be-3eb9e2bdff74",
   "metadata": {},
   "source": [
    "Now we can tokenize our data. Let's do this by splitting our data into \"words\" by whitespaces.\n",
    "\n",
    "Note that this is just a very quick and dirty way to perform tokenization. In practice, there are a lot of sophisticated methods we can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1307cf53-5a08-45aa-b3ef-fbef74479cde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:32:25.456836Z",
     "iopub.status.busy": "2022-01-26T09:32:25.456836Z",
     "iopub.status.idle": "2022-01-26T09:32:25.471849Z",
     "shell.execute_reply": "2022-01-26T09:32:25.471849Z",
     "shell.execute_reply.started": "2022-01-26T09:32:25.456836Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 202651\n",
      "\n",
      "['First', 'Citizen:', 'Before', 'we', 'proceed', 'any', 'further,', 'hear', 'me', 'speak.', 'All:', 'Speak,', 'speak.', 'First', 'Citizen:', 'You', 'are', 'all', 'resolved', 'rather', 'to', 'die', 'than', 'to', 'famish?', 'All:', 'Resolved.', 'resolved.', 'First', 'Citizen:', 'First,', 'you', 'know', 'Caius', 'Marcius', 'is', 'chief', 'enemy', 'to', 'the', 'people.', 'All:', 'We', \"know't,\", 'we', \"know't.\", 'First', 'Citizen:', 'Let', 'us', 'kill', 'him,', 'and', \"we'll\", 'have', 'corn', 'at', 'our', 'own', 'price.', \"Is't\", 'a', 'verdict?', 'All:', 'No', 'more', 'talking', \"on't;\", 'let', 'it', 'be', 'done:', 'away,', 'away!', 'Second', 'Citizen:', 'One', 'word,', 'good', 'citizens.', 'First', 'Citizen:', 'We', 'are', 'accounted', 'poor', 'citizens,', 'the', 'patricians', 'good.', 'What', 'authority', 'surfeits', 'on', 'would', 'relieve', 'us:', 'if', 'they', 'would', 'yield', 'us', 'but', 'the', 'superfluity,', 'while', 'it', 'were', 'wholesome,', 'we', 'might', 'guess', 'they', 'relieved', 'us', 'humanely;', 'but', 'they', 'think', 'we', 'are', 'too', 'dear:', 'the', 'leanness', 'that', 'afflicts', 'us,', 'the', 'object', 'of', 'our', 'misery,', 'is', 'as', 'an', 'inventory', 'to', 'particularise', 'their', 'abundance;', 'our', 'sufferance', 'is', 'a', 'gain', 'to', 'them', 'Let', 'us', 'revenge', 'this', 'with', 'our', 'pikes,', 'ere', 'we', 'become', 'rakes:', 'for', 'the', 'gods', 'know', 'I', 'speak', 'this', 'in', 'hunger', 'for', 'bread,', 'not', 'in', 'thirst', 'for', 'revenge.', 'Second', 'Citizen:', 'Would', 'you', 'proceed', 'especially', 'against', 'Caius', 'Marcius?', 'All:', 'Against', 'him', 'first:', \"he's\", 'a', 'very', 'dog', 'to', 'the', 'commonalty.', 'Second', 'Citizen:', 'Consider', 'you', 'what', 'services', 'he', 'has', 'done', 'for', 'his', 'country?', 'First', 'Citizen:', 'Very', 'well;', 'and', 'could', 'be', 'content', 'to', 'give', 'him', 'good', 'report', 'fort,', 'but', 'that', 'he', 'pays', 'himself', 'with', 'being', 'proud.', 'Second', 'Citizen:', 'Nay,', 'but', 'speak', 'not', 'maliciously.', 'First', 'Citizen:', 'I', 'say', 'unto', 'you,', 'what', 'he', 'hath', 'done', 'famously,', 'he', 'did', 'it']\n"
     ]
    }
   ],
   "source": [
    "# Naive tokenization\n",
    "tokens = text.split()\n",
    "\n",
    "print(f'Total number of tokens: {len(tokens)}\\n')\n",
    "print(tokens[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b259dea-7f3f-43f2-8925-d0c136e4d696",
   "metadata": {},
   "source": [
    "Next, let's create a vocabulary to store all the unique tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74cd73f0-f778-471e-8678-3c83147be120",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:32:25.472851Z",
     "iopub.status.busy": "2022-01-26T09:32:25.472851Z",
     "iopub.status.idle": "2022-01-26T09:32:25.487864Z",
     "shell.execute_reply": "2022-01-26T09:32:25.487864Z",
     "shell.execute_reply.started": "2022-01-26T09:32:25.472851Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 25670\n"
     ]
    }
   ],
   "source": [
    "# Create vocabulary\n",
    "vocab = list(set(tokens))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f'Number of unique tokens: {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8065f26-e233-4d77-82aa-4872d5b08dfd",
   "metadata": {},
   "source": [
    "Tensorflow/Keras has some useful utilities for easy processing of text data. \n",
    "\n",
    "Let's create a `tf.keras.layers.StringLookup` layer and convert our tokens to index values based on the vocabulary we just made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "906153fc-02a0-441b-a4e1-9285ebb28776",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:32:25.638003Z",
     "iopub.status.busy": "2022-01-26T09:32:25.638003Z",
     "iopub.status.idle": "2022-01-26T09:32:27.343596Z",
     "shell.execute_reply": "2022-01-26T09:32:27.343596Z",
     "shell.execute_reply.started": "2022-01-26T09:32:25.638003Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(202651,), dtype=int64, numpy=array([12663, 18387, 15266, ...,  1900, 23059,  7514], dtype=int64)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use tf.keras to make a lookup table (token -> id)\n",
    "ids_from_tokens = tf.keras.layers.StringLookup(vocabulary=vocab)\n",
    "\n",
    "ids = ids_from_tokens(tokens)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af8104c-bf6c-44a5-8caf-d5c575ee118f",
   "metadata": {},
   "source": [
    "In order to interpret our predictions at inference time, we need create a reverse lookup table that converts an index back into an actual token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaee3e7d-a0d4-458b-be9e-f72b86a1d7c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:32:27.344597Z",
     "iopub.status.busy": "2022-01-26T09:32:27.344597Z",
     "iopub.status.idle": "2022-01-26T09:32:27.471713Z",
     "shell.execute_reply": "2022-01-26T09:32:27.471713Z",
     "shell.execute_reply.started": "2022-01-26T09:32:27.344597Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reverse lookup\n",
    "tokens_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_tokens.get_vocabulary(), invert=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9f6485-1126-4063-8be0-d7c7c3e4cdcb",
   "metadata": {},
   "source": [
    "Next, we can use a `tf.data.Dataset` object that will allow us to conveniently batch and loop through our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3996832a-0c7f-479f-8ed3-26502c28d7a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:32:27.472713Z",
     "iopub.status.busy": "2022-01-26T09:32:27.472713Z",
     "iopub.status.idle": "2022-01-26T09:32:27.727944Z",
     "shell.execute_reply": "2022-01-26T09:32:27.727944Z",
     "shell.execute_reply.started": "2022-01-26T09:32:27.472713Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First\n",
      "Citizen:\n",
      "Before\n",
      "we\n",
      "proceed\n",
      "any\n",
      "further,\n",
      "hear\n",
      "me\n",
      "speak.\n"
     ]
    }
   ],
   "source": [
    "# Create a tf dataset object for easy batching / looping over data\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(ids_from_tokens(tokens))\n",
    "\n",
    "for ids in ids_dataset.take(10):\n",
    "    print(tokens_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad5b417-0e86-46f7-a87a-94fe5d8e0ce8",
   "metadata": {},
   "source": [
    "Next, we need to pre-define the length of our sequences for batching and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aede62a-8dc6-4778-892e-a99d29999ab5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:32:27.728945Z",
     "iopub.status.busy": "2022-01-26T09:32:27.728945Z",
     "iopub.status.idle": "2022-01-26T09:32:27.759974Z",
     "shell.execute_reply": "2022-01-26T09:32:27.759974Z",
     "shell.execute_reply.started": "2022-01-26T09:32:27.728945Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'First' b'Citizen:' b'Before' b'we' b'proceed' b'any' b'further,'\n",
      " b'hear' b'me' b'speak.' b'All:' b'Speak,' b'speak.' b'First' b'Citizen:'\n",
      " b'You' b'are' b'all' b'resolved' b'rather' b'to'], shape=(21,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "seq_length = 20\n",
    "examples_per_epoch = len(tokens)\n",
    "\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True) # +1 because we want to predict the next word\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "    print(tokens_from_ids(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7633ed10-041a-41e9-b5dd-f5cd7c364d80",
   "metadata": {},
   "source": [
    "Let's create a function that can split a sequence into input and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f46259b-4bcb-47bd-826c-5fc7dc60a01a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:32:27.989181Z",
     "iopub.status.busy": "2022-01-26T09:32:27.989181Z",
     "iopub.status.idle": "2022-01-26T09:32:27.999190Z",
     "shell.execute_reply": "2022-01-26T09:32:27.999190Z",
     "shell.execute_reply.started": "2022-01-26T09:32:27.989181Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Tensorflow', 'is', 'very'], ['is', 'very', 'cool.'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "split_input_target('Tensorflow is very cool.'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc83437b-d45f-4752-829d-9dbf4e4c2182",
   "metadata": {},
   "source": [
    "Not let's apply the `split_input_target` function on all our sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c1c7e49-0a33-4b6d-80b3-1bb4ae941a04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:32:28.449600Z",
     "iopub.status.busy": "2022-01-26T09:32:28.449600Z",
     "iopub.status.idle": "2022-01-26T09:32:28.541682Z",
     "shell.execute_reply": "2022-01-26T09:32:28.541682Z",
     "shell.execute_reply.started": "2022-01-26T09:32:28.449600Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : tf.Tensor(\n",
      "[b'First' b'Citizen:' b'Before' b'we' b'proceed' b'any' b'further,'\n",
      " b'hear' b'me' b'speak.' b'All:' b'Speak,' b'speak.' b'First' b'Citizen:'\n",
      " b'You' b'are' b'all' b'resolved' b'rather'], shape=(20,), dtype=string)\n",
      "Target: tf.Tensor(\n",
      "[b'Citizen:' b'Before' b'we' b'proceed' b'any' b'further,' b'hear' b'me'\n",
      " b'speak.' b'All:' b'Speak,' b'speak.' b'First' b'Citizen:' b'You' b'are'\n",
      " b'all' b'resolved' b'rather' b'to'], shape=(20,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", tokens_from_ids(input_example))\n",
    "    print(\"Target:\", tokens_from_ids(target_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60aa66f-3128-45a8-a7c5-a261db23ac0c",
   "metadata": {},
   "source": [
    "Let's finalize the construction of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55be0b14-e455-406b-9d24-89d2b4e77834",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:32:29.210289Z",
     "iopub.status.busy": "2022-01-26T09:32:29.210289Z",
     "iopub.status.idle": "2022-01-26T09:32:29.230306Z",
     "shell.execute_reply": "2022-01-26T09:32:29.230306Z",
     "shell.execute_reply.started": "2022-01-26T09:32:29.210289Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 20), (256, 20)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "batch_size = 256\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "buffer_size = 1000\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True) # Shuffle and batch\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd02d518-0b75-4996-8ff2-d5ffdf2092aa",
   "metadata": {},
   "source": [
    "Now, let's build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ca9b56e-0417-471f-b7de-9cf9fbe2a742",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:32:49.377564Z",
     "iopub.status.busy": "2022-01-26T09:32:49.377564Z",
     "iopub.status.idle": "2022-01-26T09:32:49.443624Z",
     "shell.execute_reply": "2022-01-26T09:32:49.443624Z",
     "shell.execute_reply.started": "2022-01-26T09:32:49.377564Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 256)         6571776   \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, None, 512)         393728    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 25671)       13169223  \n",
      "=================================================================\n",
      "Total params: 20,134,727\n",
      "Trainable params: 20,134,727\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 512\n",
    "\n",
    "# Word-level RNN\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size+1, embedding_dim)) # +1 for the unknown token\n",
    "model.add(tf.keras.layers.SimpleRNN(rnn_units, return_sequences=True)) # return outputs at every time step\n",
    "model.add(tf.keras.layers.Dense(vocab_size+1)) # notice how theres no softmax here (you can put it in the loss function!)\n",
    "          \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d5c079-9a7e-4333-a00e-27e408f05eea",
   "metadata": {},
   "source": [
    "Let's define the loss function and choose our optimizer.\n",
    "\n",
    "Also, you can use checkpointing to save your model at regular intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54d4ef17-e69a-4e47-8a48-b179fe2e5f03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:32:51.450458Z",
     "iopub.status.busy": "2022-01-26T09:32:51.450458Z",
     "iopub.status.idle": "2022-01-26T09:32:51.457463Z",
     "shell.execute_reply": "2022-01-26T09:32:51.457463Z",
     "shell.execute_reply.started": "2022-01-26T09:32:51.450458Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='./training_checkpoints/ckpt_{epoch}',\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6245a54f-1f8f-4cce-90dc-2e542d5cda5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:45:57.913214Z",
     "iopub.status.busy": "2022-01-26T09:45:57.913214Z",
     "iopub.status.idle": "2022-01-26T09:46:45.841942Z",
     "shell.execute_reply": "2022-01-26T09:46:45.841942Z",
     "shell.execute_reply.started": "2022-01-26T09:45:57.913214Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "37/37 [==============================] - 2s 42ms/step - loss: 0.5868\n",
      "Epoch 2/20\n",
      "37/37 [==============================] - 2s 42ms/step - loss: 0.5704\n",
      "Epoch 3/20\n",
      "37/37 [==============================] - 2s 45ms/step - loss: 0.5556\n",
      "Epoch 4/20\n",
      "37/37 [==============================] - 2s 45ms/step - loss: 0.5420\n",
      "Epoch 5/20\n",
      "37/37 [==============================] - 2s 49ms/step - loss: 0.5288\n",
      "Epoch 6/20\n",
      "37/37 [==============================] - 2s 45ms/step - loss: 0.5148\n",
      "Epoch 7/20\n",
      "37/37 [==============================] - 2s 45ms/step - loss: 0.5024\n",
      "Epoch 8/20\n",
      "37/37 [==============================] - 2s 47ms/step - loss: 0.4908\n",
      "Epoch 9/20\n",
      "37/37 [==============================] - 2s 46ms/step - loss: 0.4788\n",
      "Epoch 10/20\n",
      "37/37 [==============================] - 2s 46ms/step - loss: 0.4677\n",
      "Epoch 11/20\n",
      "37/37 [==============================] - 2s 42ms/step - loss: 0.4566\n",
      "Epoch 12/20\n",
      "37/37 [==============================] - 2s 51ms/step - loss: 0.4447\n",
      "Epoch 13/20\n",
      "37/37 [==============================] - 2s 52ms/step - loss: 0.4352\n",
      "Epoch 14/20\n",
      "37/37 [==============================] - 2s 50ms/step - loss: 0.4254\n",
      "Epoch 15/20\n",
      "37/37 [==============================] - 2s 47ms/step - loss: 0.4157\n",
      "Epoch 16/20\n",
      "37/37 [==============================] - 2s 47ms/step - loss: 0.4060\n",
      "Epoch 17/20\n",
      "37/37 [==============================] - 2s 47ms/step - loss: 0.3956\n",
      "Epoch 18/20\n",
      "37/37 [==============================] - 2s 46ms/step - loss: 0.3862\n",
      "Epoch 19/20\n",
      "37/37 [==============================] - 2s 47ms/step - loss: 0.3769\n",
      "Epoch 20/20\n",
      "37/37 [==============================] - 2s 48ms/step - loss: 0.3680\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "history = model.fit(dataset, epochs=epochs, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2cd0eb-1391-4fb4-9e02-0f94f2493254",
   "metadata": {},
   "source": [
    "Before we generate text, let's briefly describe a common parameter used to control the \"variance\" of our logits called **temperature**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab8c348c-fea0-44d5-b29f-a3ca0e7d2567",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:46:46.510766Z",
     "iopub.status.busy": "2022-01-26T09:46:46.510766Z",
     "iopub.status.idle": "2022-01-26T09:46:46.526781Z",
     "shell.execute_reply": "2022-01-26T09:46:46.526781Z",
     "shell.execute_reply.started": "2022-01-26T09:46:46.510766Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  logits = [ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763]\n",
      "temp 0.5 = [0.80087103 0.00914764 0.0108121  0.00363668 0.17553254]\n",
      "temp 1.0 = [0.56862917 0.06077185 0.06606977 0.0383178  0.26621141]\n",
      "temp 2.0 = [0.38290729 0.12517866 0.13052103 0.09939839 0.26199464]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "logits = np.random.normal(loc=0, scale=1, size=5)\n",
    "\n",
    "print(f'  logits = {logits}')\n",
    "print(f'temp 0.5 = {tf.nn.softmax(logits/0.5).numpy()}')\n",
    "print(f'temp 1.0 = {tf.nn.softmax(logits/1.0).numpy()}')\n",
    "print(f'temp 2.0 = {tf.nn.softmax(logits/2.0).numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dc624a-a962-401f-8895-8188d2bc3595",
   "metadata": {},
   "source": [
    "Notice what happens to the probabilities as we divide our logits by the temperature parameter. Lowering the temperature below 1.0 decreases the variance of the distribution, while increasing it also increases the variance. \n",
    "\n",
    "This will allow us to control how \"wild\" the generated text is when we sample it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c473dc91-3201-4fd5-b50b-1691e3743524",
   "metadata": {},
   "source": [
    "Now that everything is done, we can generate new text by passing a prompt into our model and randomly sampling from the final output (which is a distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "782d5759-aa87-441b-a3b3-7937b1ca3a50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:47:40.383525Z",
     "iopub.status.busy": "2022-01-26T09:47:40.382525Z",
     "iopub.status.idle": "2022-01-26T09:47:41.054134Z",
     "shell.execute_reply": "2022-01-26T09:47:41.054134Z",
     "shell.execute_reply.started": "2022-01-26T09:47:40.383525Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: Well, Here, why, nature is framed, and to be sufficiency, as your sea York And doubt thou choose custom. instruct,\n"
     ]
    }
   ],
   "source": [
    "temperature = 1.75 # Tuneable\n",
    "prompt = 'ROMEO:'\n",
    "\n",
    "gen_len = 20\n",
    "\n",
    "for i in range(gen_len):\n",
    "\n",
    "    output = model(tf.expand_dims(ids_from_tokens(prompt.split()), axis=0))\n",
    "    output = output[:, -1, :]\n",
    "    output = output/temperature\n",
    "    output = tf.random.categorical(output, num_samples=1)\n",
    "    output = tf.squeeze(output, axis=-1)\n",
    "\n",
    "    output_text = tokens_from_ids(output)\n",
    "    output_text = output_text.numpy()[0].decode('utf-8')\n",
    "    \n",
    "    prompt = prompt + ' ' + output_text\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d542c254-0d2c-46de-8d03-c55ccbbdabec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T09:47:50.839029Z",
     "iopub.status.busy": "2022-01-26T09:47:50.839029Z",
     "iopub.status.idle": "2022-01-26T09:47:51.750855Z",
     "shell.execute_reply": "2022-01-26T09:47:51.750855Z",
     "shell.execute_reply.started": "2022-01-26T09:47:50.839029Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: Amen, amen! that, then and tell me, do your cousin's death? HENRY BOLINGBROKE: Sir Dear Harry, take me Master much:\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained\n",
    "new_model = tf.keras.models.load_model('models/word-rnn-shakespeare_v1.h5')\n",
    "\n",
    "temperature = 1.75 # Tuneable\n",
    "prompt = 'ROMEO:'\n",
    "\n",
    "gen_len = 20\n",
    "\n",
    "for i in range(gen_len):\n",
    "\n",
    "    output = new_model(tf.expand_dims(ids_from_tokens(prompt.split()), axis=0))\n",
    "    output = output[:, -1, :]\n",
    "    output = output/temperature\n",
    "    output = tf.random.categorical(output, num_samples=1)\n",
    "    output = tf.squeeze(output, axis=-1)\n",
    "\n",
    "    output_text = tokens_from_ids(output)\n",
    "    output_text = output_text.numpy()[0].decode('utf-8')\n",
    "    \n",
    "    prompt = prompt + ' ' + output_text\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3d198e-e9d7-4f60-8762-5f8c988f7aa1",
   "metadata": {},
   "source": [
    "## Refinements and Extensions\n",
    "\n",
    "- Keep increasing epochs.\n",
    "\n",
    "\n",
    "- Refine pre-processing.\n",
    "\n",
    "\n",
    "- Refine post-processing.\n",
    "\n",
    "\n",
    "- Tune hyperparameters.\n",
    "\n",
    "\n",
    "- Try different RNNs.\n",
    "\n",
    "\n",
    "- Try a different dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9655a9d-4aba-4024-a306-a9a0cf02a068",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "---\n",
    "\n",
    "[1] https://www.tensorflow.org/text/tutorials/text_generation\n",
    "\n",
    "[2] https://www.tensorflow.org/tutorials/keras/text_classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-2.6]",
   "language": "python",
   "name": "conda-env-tf-2.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
